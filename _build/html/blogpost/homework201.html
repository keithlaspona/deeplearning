
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Mastering Momentum: A Deep Dive into Adam’s Adaptive Mechanics &#8212; Deep Learning Portfolio</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'blogpost/homework201';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Laboratory Task 6 - Implementation of a CNN Architecture" href="../notebooks/Laboratory_6.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.gif" class="logo__image only-light" alt="Deep Learning Portfolio - Home"/>
    <script>document.write(`<img src="../_static/logo.gif" class="logo__image only-dark" alt="Deep Learning Portfolio - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    DS413 – Elective 4
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory Exercises</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notebooks/Laboratory_1.html"><strong>Laboratory Task 1</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/Laboratory_2.html"><strong>Laboratory Task 2 - Single Forward Pass and Error Analysis</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/Laboratory_3.html"><strong>Laboratory Task 3 - Implementing Forward and Backward Propagation</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/Laboratory_4.html"><strong>Laboratory Task 4 - Training Linear Regression Model in PyTorch</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/Laboratory_5.html"><strong>Laboratory Task 5 - PyTorch Basics</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/Laboratory_6.html"><strong>Laboratory Task 6 - Implementation of a CNN Architecture</strong></a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Blogpost</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#"><strong>Mastering Momentum: A Deep Dive into Adam’s Adaptive Mechanics</strong></a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/keithlaspona/deeplearning" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/keithlaspona/deeplearning/issues/new?title=Issue%20on%20page%20%2Fblogpost/homework201.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/blogpost/homework201.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Mastering Momentum: A Deep Dive into Adam’s Adaptive Mechanics</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-intuition-a-tale-of-two-hikers">1. The Intuition: A Tale of Two Hikers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sgd-hiker">The SGD Hiker</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-adam-hiker">The Adam Hiker</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mathematics-under-the-hood">2. The Mathematics: Under the Hood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-moment-momentum-m-t">2.1 First Moment: Momentum <span class="math notranslate nohighlight">\(m_t\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#second-moment-variance-v-t">2.2 Second Moment: Variance <span class="math notranslate nohighlight">\(v_t\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cold-start-problem-bias-correction">2.3 The “Cold Start” Problem: Bias Correction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-update-rule">2.4 The Update Rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-adam-wins-saddle-points-sparse-gradients">3. Why Adam Wins: Saddle Points &amp; Sparse Gradients</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#escaping-saddle-points">3.1 Escaping Saddle Points</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-sparse-gradients">3.2 Handling Sparse Gradients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-experiment-adam-vs-sgd-on-mnist">4. The Experiment: Adam vs SGD on MNIST</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#experimental-setup">4.1 Experimental Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#results">4.3 Results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-observed">4.4 What We Observed</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-guide-hyperparameter-tuning-for-adam">5. Practical Guide: Hyperparameter Tuning for Adam</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-generalization-gap-warning">The <strong>“Generalization Gap”</strong> Warning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="mastering-momentum-a-deep-dive-into-adam-s-adaptive-mechanics">
<h1><strong>Mastering Momentum: A Deep Dive into Adam’s Adaptive Mechanics</strong><a class="headerlink" href="#mastering-momentum-a-deep-dive-into-adam-s-adaptive-mechanics" title="Link to this heading">#</a></h1>
<p><strong>Author:</strong> Keith Laspoña<br />
<em>8 min read · Deep Learning · MNIST Experiment</em> <br></p>
<p>In the vast landscape of Deep Learning, building the architecture is only half the battle. The other half is figuring out how to teach that architecture to learn. Enter the <strong>optimizer</strong>: the engine that drives the neural network down the loss landscape toward convergence.</p>
<p>While Stochastic Gradient Descent (SGD) is the grandfather of optimizers, there is one name that dominates nearly every modern research paper, Kaggle competition, and production model: <strong>Adam</strong>.</p>
<p>But why is Adam (Adaptive Moment Estimation) the default choice? Is it simply faster, or is there something more sophisticated happening under the hood? In this notebook, we will:</p>
<ul class="simple">
<li><p>Deconstruct the Adam algorithm,</p></li>
<li><p>Explore the bias correction math that saves it from early failure, and</p></li>
<li><p>Run a small experiment on <strong>MNIST</strong> to visualize how Adam compares to vanilla SGD in practice.</p></li>
</ul>
<br>
<section id="the-intuition-a-tale-of-two-hikers">
<h2>1. The Intuition: A Tale of Two Hikers<a class="headerlink" href="#the-intuition-a-tale-of-two-hikers" title="Link to this heading">#</a></h2>
<p>To understand Adam, imagine two hikers trying to get to the bottom of a dark canyon (the global minimum).</p>
<section id="the-sgd-hiker">
<h3>The SGD Hiker<a class="headerlink" href="#the-sgd-hiker" title="Link to this heading">#</a></h3>
<p>He takes steps of a fixed size. If the slope is steep, he might overshoot. If the terrain flattens out (a plateau), his fixed steps become painfully slow, and he might get stuck. He only cares about the <strong>current slope</strong>.</p>
</section>
<section id="the-adam-hiker">
<h3>The Adam Hiker<a class="headerlink" href="#the-adam-hiker" title="Link to this heading">#</a></h3>
<p>She has a “memory” of her past speed (<strong>Momentum</strong>) and she adjusts her stride length based on the terrain’s roughness (<strong>Adaptive Learning Rate</strong>):</p>
<ul class="simple">
<li><p>If she has been moving fast in one direction, she keeps going that way (momentum).</p></li>
<li><p>If the terrain is steep and unpredictable, she takes smaller, cautious steps.</p></li>
<li><p>If the terrain is flat and consistent, she takes massive leaps to cross it quickly.</p></li>
</ul>
<p>Adam is essentially <strong>SGD with a brain</strong>. It combines two powerful ideas:</p>
<ul class="simple">
<li><p><strong>Momentum</strong> (from SGD with momentum), and</p></li>
<li><p><strong>RMSProp</strong> (Root Mean Square Propagation, which adapts learning rates based on gradient magnitudes).</p></li>
</ul>
<br>
</section>
</section>
<section id="the-mathematics-under-the-hood">
<h2>2. The Mathematics: Under the Hood<a class="headerlink" href="#the-mathematics-under-the-hood" title="Link to this heading">#</a></h2>
<p>Adam tracks two variables, or <strong>moments</strong>, for every single weight in your network. Let <span class="math notranslate nohighlight">\(g_t\)</span> be the gradient at time step <span class="math notranslate nohighlight">\(t\)</span>.</p>
<section id="first-moment-momentum-m-t">
<h3>2.1 First Moment: Momentum <span class="math notranslate nohighlight">\(m_t\)</span><a class="headerlink" href="#first-moment-momentum-m-t" title="Link to this heading">#</a></h3>
<p>This tracks the moving average of the gradient. It tells us <strong>the direction to go</strong>:</p>
<div class="math notranslate nohighlight">
\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(m_t\)</span>: first moment estimate (like velocity)</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1\)</span>: decay rate, typically 0.9 (keep 90% of the past, 10% new gradient)</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="second-moment-variance-v-t">
<h3>2.2 Second Moment: Variance <span class="math notranslate nohighlight">\(v_t\)</span><a class="headerlink" href="#second-moment-variance-v-t" title="Link to this heading">#</a></h3>
<p>This tracks the moving average of the <strong>squared</strong> gradient. It tells us <strong>the magnitude or volatility</strong> of the slope:</p>
<div class="math notranslate nohighlight">
\[
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(v_t\)</span>: second moment estimate (like variance)</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_2\)</span>: decay rate, typically 0.999</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="the-cold-start-problem-bias-correction">
<h3>2.3 The “Cold Start” Problem: Bias Correction<a class="headerlink" href="#the-cold-start-problem-bias-correction" title="Link to this heading">#</a></h3>
<p>Since <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(v\)</span> are initialized as vectors of zeros, the first updates are biased toward zero.<br />
For example, if <span class="math notranslate nohighlight">\(m_0 = 0\)</span> and <span class="math notranslate nohighlight">\(\beta_1 = 0.9\)</span>:</p>
<div class="math notranslate nohighlight">
\[
m_1 = 0.1 \cdot g_1
\]</div>
<p>To correct this, Adam uses <strong>bias-corrected estimates</strong>:</p>
<div class="math notranslate nohighlight">
\[
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}
\]</div>
<div class="math notranslate nohighlight">
\[
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
\]</div>
<p>At <span class="math notranslate nohighlight">\(t = 1\)</span>, dividing by <span class="math notranslate nohighlight">\(1 - 0.9^1 = 0.1\)</span> restores the scale of the moment estimate.<br />
As <span class="math notranslate nohighlight">\(t \to \infty\)</span>, <span class="math notranslate nohighlight">\(\beta^t \to 0\)</span>, so the correction eventually disappears.</p>
</section>
<hr class="docutils" />
<section id="the-update-rule">
<h3>2.4 The Update Rule<a class="headerlink" href="#the-update-rule" title="Link to this heading">#</a></h3>
<p>Finally, we update the parameters ( \theta ):</p>
<div class="math notranslate nohighlight">
\[
\theta_{t+1} = \theta_t - \frac{\eta \cdot \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\eta\)</span>: global learning rate (often 0.001)</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span>: small constant (e.g., <span class="math notranslate nohighlight">\(10^{-8}\)</span>) for numerical stability</p></li>
</ul>
<p>The combination of <strong>momentum</strong>, <strong>adaptive scaling</strong>, and <strong>bias correction</strong> is what makes Adam powerful and stable, especially during the early stages of training.</p>
<br>
</section>
</section>
<section id="why-adam-wins-saddle-points-sparse-gradients">
<h2>3. Why Adam Wins: Saddle Points &amp; Sparse Gradients<a class="headerlink" href="#why-adam-wins-saddle-points-sparse-gradients" title="Link to this heading">#</a></h2>
<p>The math above gives Adam two superpowers that standard SGD lacks.</p>
<section id="escaping-saddle-points">
<h3>3.1 Escaping Saddle Points<a class="headerlink" href="#escaping-saddle-points" title="Link to this heading">#</a></h3>
<p>A <strong>saddle point</strong> is a region where the loss surface is flat in one dimension but curves up in another (like a horse saddle). In high-dimensional deep learning, local minima are rare; getting stuck on saddle points or flat regions is the real danger.</p>
<ul class="simple">
<li><p><strong>SGD</strong> often stalls here because the gradient is near zero in all directions.</p></li>
<li><p><strong>Adam</strong> uses its stored momentum <span class="math notranslate nohighlight">\(m_t\)</span> to “roll” through flat areas, maintaining velocity even when the current gradient suggests stopping.</p></li>
</ul>
</section>
<section id="handling-sparse-gradients">
<h3>3.2 Handling Sparse Gradients<a class="headerlink" href="#handling-sparse-gradients" title="Link to this heading">#</a></h3>
<p>In problems like NLP or Computer Vision (e.g., MNIST), some features (pixels or words) appear very rarely.</p>
<ul class="simple">
<li><p><strong>SGD</strong>: Updates all parameters with the same learning rate. Rare features get updated very infrequently, so they may not learn well.</p></li>
<li><p><strong>Adam</strong>: Dividing by <span class="math notranslate nohighlight">\(\sqrt{\hat{v}_t}\)</span> acts like a normalizer.<br />
If a parameter has rarely seen large gradients (low <span class="math notranslate nohighlight">\(v_t\)</span>), the denominator is small, increasing the <strong>effective step size</strong> for that parameter.</p></li>
</ul>
<p>Adam essentially says:</p>
<blockquote>
<div><p><em>“We haven’t seen this feature often, so when we do, let’s learn a LOT from it.”</em></p>
</div></blockquote>
<br>
</section>
</section>
<section id="the-experiment-adam-vs-sgd-on-mnist">
<h2>4. The Experiment: Adam vs SGD on MNIST<a class="headerlink" href="#the-experiment-adam-vs-sgd-on-mnist" title="Link to this heading">#</a></h2>
<p>To ground the theory, we now compare <strong>Adam</strong> and <strong>vanilla SGD</strong> on the classic <strong>MNIST handwritten digits</strong> dataset. The goal is not to break any accuracy records, but to directly observe how the choice of optimizer affects <strong>convergence speed</strong> and <strong>stability</strong>.</p>
<section id="experimental-setup">
<h3>4.1 Experimental Setup<a class="headerlink" href="#experimental-setup" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Dataset</strong>: MNIST (60,000 training images, 10,000 test images, 28×28 grayscale digits)</p></li>
<li><p><strong>Preprocessing</strong>:</p>
<ul>
<li><p>Normalize pixel values to ([0, 1])</p></li>
<li><p>Flatten 28×28 images into 784-dimensional vectors</p></li>
</ul>
</li>
<li><p><strong>Model (Simple MLP)</strong>:</p>
<ul>
<li><p>Input: 784</p></li>
<li><p>Dense(128, ReLU)</p></li>
<li><p>Dropout(0.2)</p></li>
<li><p>Dense(10, Softmax)</p></li>
</ul>
</li>
<li><p><strong>Loss</strong>: Categorical cross-entropy</p></li>
<li><p><strong>Batch size</strong>: 128</p></li>
<li><p><strong>Epochs</strong>: 10</p></li>
<li><p><strong>Optimizers</strong>:</p>
<ul>
<li><p><strong>SGD</strong> with learning rate = 0.01</p></li>
<li><p><strong>Adam</strong> with learning rate = 0.001</p></li>
</ul>
</li>
</ul>
<blockquote>
<div><p><strong>Key idea:</strong> We keep the model, data, and training loop <strong>identical</strong>, and only swap the optimizer. This isolates the effect of the optimization algorithm itself.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 4.2 Implementation: Training with Adam vs SGD</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras</span><span class="w"> </span><span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">models</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">mnist</span>

<span class="c1"># Set random seeds for reproducibility</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Load and preprocess MNIST</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_test</span>  <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="c1"># Flatten 28x28 → 784</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span>
<span class="n">x_test</span>  <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span>

<span class="c1"># One-hot encode labels</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
<span class="n">y_test</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">build_mlp_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,)),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">),</span>
    <span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2. Train with two different optimizers</span>
<span class="n">optimizers</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;SGD&quot;</span><span class="p">:</span>  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>
    <span class="s2">&quot;Adam&quot;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">histories</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">opt_name</span><span class="p">,</span> <span class="n">opt</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training with </span><span class="si">{</span><span class="n">opt_name</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">build_mlp_model</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span>
        <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;categorical_crossentropy&quot;</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
        <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>

    <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">opt_name</span><span class="si">}</span><span class="s2"> test accuracy: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Save full training history + test accuracy</span>
    <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;test_accuracy&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_acc</span>
    <span class="n">histories</span><span class="p">[</span><span class="n">opt_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training with SGD...
Epoch 1/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - accuracy: 0.6370 - loss: 1.3304 - val_accuracy: 0.8660 - val_loss: 0.6782
Epoch 2/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 3ms/step - accuracy: 0.8175 - loss: 0.6762 - val_accuracy: 0.8975 - val_loss: 0.4461
Epoch 3/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 3ms/step - accuracy: 0.8485 - loss: 0.5386 - val_accuracy: 0.9115 - val_loss: 0.3695
Epoch 4/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 3ms/step - accuracy: 0.8664 - loss: 0.4754 - val_accuracy: 0.9175 - val_loss: 0.3304
Epoch 5/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 3ms/step - accuracy: 0.8770 - loss: 0.4346 - val_accuracy: 0.9218 - val_loss: 0.3055
Epoch 6/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 3ms/step - accuracy: 0.8841 - loss: 0.4073 - val_accuracy: 0.9250 - val_loss: 0.2869
Epoch 7/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 3ms/step - accuracy: 0.8900 - loss: 0.3844 - val_accuracy: 0.9277 - val_loss: 0.2731
Epoch 8/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 3ms/step - accuracy: 0.8952 - loss: 0.3672 - val_accuracy: 0.9313 - val_loss: 0.2612
Epoch 9/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 3ms/step - accuracy: 0.8985 - loss: 0.3552 - val_accuracy: 0.9330 - val_loss: 0.2515
Epoch 10/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 3ms/step - accuracy: 0.9023 - loss: 0.3430 - val_accuracy: 0.9350 - val_loss: 0.2428
SGD test accuracy: 0.9241

Training with Adam...
Epoch 1/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - accuracy: 0.8776 - loss: 0.4232 - val_accuracy: 0.9532 - val_loss: 0.1732
Epoch 2/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - accuracy: 0.9408 - loss: 0.2056 - val_accuracy: 0.9650 - val_loss: 0.1275
Epoch 3/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - accuracy: 0.9548 - loss: 0.1544 - val_accuracy: 0.9685 - val_loss: 0.1104
Epoch 4/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - accuracy: 0.9628 - loss: 0.1269 - val_accuracy: 0.9727 - val_loss: 0.0934
Epoch 5/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - accuracy: 0.9675 - loss: 0.1075 - val_accuracy: 0.9752 - val_loss: 0.0835
Epoch 6/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - accuracy: 0.9724 - loss: 0.0932 - val_accuracy: 0.9757 - val_loss: 0.0813
Epoch 7/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - accuracy: 0.9749 - loss: 0.0816 - val_accuracy: 0.9772 - val_loss: 0.0764
Epoch 8/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - accuracy: 0.9778 - loss: 0.0728 - val_accuracy: 0.9797 - val_loss: 0.0703
Epoch 9/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - accuracy: 0.9808 - loss: 0.0644 - val_accuracy: 0.9792 - val_loss: 0.0705
Epoch 10/10
<span class=" -Color -Color-Bold">422/422</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - accuracy: 0.9808 - loss: 0.0613 - val_accuracy: 0.9815 - val_loss: 0.0666
Adam test accuracy: 0.9774
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 4.2 – Plotting Learning Curves</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">opt_name</span><span class="p">,</span> <span class="n">hist</span> <span class="ow">in</span> <span class="n">histories</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist</span><span class="p">[</span><span class="s2">&quot;val_accuracy&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">opt_name</span><span class="si">}</span><span class="s2"> (val)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Validation Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Adam vs SGD on MNIST (Validation Accuracy)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">opt_name</span><span class="p">,</span> <span class="n">hist</span> <span class="ow">in</span> <span class="n">histories</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">opt_name</span><span class="si">}</span><span class="s2"> (val)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Validation Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Adam vs SGD on MNIST (Validation Loss)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Print final test accuracies</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Final test accuracies:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">opt_name</span><span class="p">,</span> <span class="n">hist</span> <span class="ow">in</span> <span class="n">histories</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">opt_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">hist</span><span class="p">[</span><span class="s1">&#39;test_accuracy&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a7e1525ee8ca59f2f152ac431320a58c9f990089a3b3cf4346db118bdbc13b73.png" src="../_images/a7e1525ee8ca59f2f152ac431320a58c9f990089a3b3cf4346db118bdbc13b73.png" />
<img alt="../_images/2f199ddca28f0554a532d2960013d1f7b43276dc9f695fe74f8ec40552fce4c6.png" src="../_images/2f199ddca28f0554a532d2960013d1f7b43276dc9f695fe74f8ec40552fce4c6.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final test accuracies:
SGD: 0.9241
Adam: 0.9774
</pre></div>
</div>
</div>
</div>
</section>
<section id="results">
<h3>4.3 Results<a class="headerlink" href="#results" title="Link to this heading">#</a></h3>
<p>Below are the exact results produced from my experiment:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Optimizer</p></th>
<th class="head"><p>Val Accuracy (Epoch 1)</p></th>
<th class="head"><p>Best Val Accuracy (10 epochs)</p></th>
<th class="head"><p>Test Accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>SGD</strong> (lr = 0.01)</p></td>
<td><p>0.8660</p></td>
<td><p>0.9350</p></td>
<td><p>0.9241</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Adam</strong> (lr = 0.001)</p></td>
<td><p>0.9532</p></td>
<td><p>0.9815</p></td>
<td><p>0.9774</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="interpretation">
<h3>Interpretation<a class="headerlink" href="#interpretation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Adam clearly starts with much higher validation accuracy</strong> and reaches &gt;95% in the very first epoch.</p></li>
<li><p><strong>SGD starts slower</strong>, reaching only ~86.6% in epoch 1 and requiring more epochs to stabilize.</p></li>
<li><p>After 10 epochs, both optimizers perform competitively, but:</p>
<ul>
<li><p>Adam achieves a higher overall validation accuracy and final test accuracy.</p></li>
<li><p>SGD is more gradual and slower to improve.</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-we-observed">
<h3>4.4 What We Observed<a class="headerlink" href="#what-we-observed" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p><strong>Adam converges significantly faster in the early epochs</strong><br />
Adam reached a validation accuracy of <strong>95.32% in the first epoch</strong>, while SGD only reached <strong>86.60%</strong>. This validates Adam’s advantage during the early stages of training: adaptive learning rates and bias-corrected momentum enable it to take larger and more confident optimization steps.</p></li>
<li><p><strong>Adam maintains more stable learning curves</strong><br />
The validation accuracy for Adam increases smoothly across epochs, while SGD displays more gradual improvement. This difference reflects how Adam stabilizes updates with momentum, reducing oscillations and adapting more quickly to changes in the loss landscape.</p></li>
<li><p><strong>Adam achieves a higher final performance in this experiment</strong><br />
After 10 epochs:</p>
<ul class="simple">
<li><p>Adam achieved a test accuracy of <strong>0.9774</strong></p></li>
<li><p>SGD finished at <strong>0.9241</strong></p></li>
</ul>
<p>In this setup, Adam not only converged sooner but also achieved a better final classification performance on MNIST.</p>
</li>
</ol>
<blockquote>
<div><p><strong>Takeaway for tuning:</strong> If training speed, stability, and quicker convergence are priorities (e.g., limited compute time), Adam is usually the better default choice. SGD remains competitive but typically requires more epochs and tuning to match Adam’s early performance.</p>
</div></blockquote>
</section>
</section>
<section id="practical-guide-hyperparameter-tuning-for-adam">
<h2>5. Practical Guide: Hyperparameter Tuning for Adam<a class="headerlink" href="#practical-guide-hyperparameter-tuning-for-adam" title="Link to this heading">#</a></h2>
<p>While Adam is famous for working “out of the box,” you may sometimes need to tune it.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>When to Tune?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Learning Rate (α)</strong></p></td>
<td><p>0.001</p></td>
<td><p>The most important knob. If loss diverges, decrease it. If loss plateaus too early, try increasing or using a scheduler.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>β₁ (Momentum)</strong></p></td>
<td><p>0.9</p></td>
<td><p>Tune for instability. If your loss curve is extremely jittery, increasing this to 0.95 or 0.99 can smooth it out.</p></td>
</tr>
<tr class="row-even"><td><p><strong>β₂ (RMSProp part)</strong></p></td>
<td><p>0.999</p></td>
<td><p>Rarely needs tuning. Lowering it (e.g., to 0.99) can help in non-stationary problems.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>ε (Epsilon)</strong></p></td>
<td><p>1e-7</p></td>
<td><p>Important for mixed-precision training (float16). Increase to 1e-4 to avoid NaNs if necessary.</p></td>
</tr>
</tbody>
</table>
</div>
<section id="the-generalization-gap-warning">
<h3>The <strong>“Generalization Gap”</strong> Warning<a class="headerlink" href="#the-generalization-gap-warning" title="Link to this heading">#</a></h3>
<p>A common critique is that Adam converges too fast to a <strong>sharp minimum</strong>, whereas SGD (with momentum) often finds a wider, flatter minimum that generalizes better to new data.</p>
<p><strong>Pro tip:</strong><br />
If you are squeezing out the final 0.1% accuracy for a competition, a common trick is:</p>
<ol class="arabic simple">
<li><p>Train with <strong>Adam</strong> at the start for fast convergence.</p></li>
<li><p>Then <strong>switch to SGD with momentum</strong> later in training once the model is close to a good solution.</p></li>
</ol>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Adam’s strength does not come from a mysterious trick—its power comes from the combination of <strong>adaptive learning rates</strong> and <strong>momentum-based updates</strong>. By adjusting the step size for every parameter and correcting for bias during early iterations, Adam is able to navigate deep learning loss landscapes more efficiently than vanilla SGD.</p>
<p>Our MNIST experiment confirmed this behavior:</p>
<ul class="simple">
<li><p>Adam reached high accuracy <strong>very quickly</strong>, achieving over 95% validation accuracy in the first epoch.</p></li>
<li><p>SGD improved more slowly and required more training to reach competitive accuracy.</p></li>
<li><p>Adam also achieved a higher final test accuracy in this setup.</p></li>
</ul>
<p>In practical deep learning workflows, this translates to:</p>
<ul class="simple">
<li><p>Faster experimentation and model iteration,</p></li>
<li><p>Less sensitivity to initial hyperparameter choice, and</p></li>
<li><p>More stable learning dynamics during the early phases of training.</p></li>
</ul>
<p>While SGD remains useful for some generalization-focused tasks or fine-tuning, Adam’s performance and stability make it a powerful default choice for many real-world applications. Understanding its mechanics—combined with experiments like this—helps build intuition for optimization and hyperparameter tuning in deep learning. <br></p>
</section>
<hr class="docutils" />
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Kingma, D. P., &amp; Ba, J. (2015).<br />
<em>Adam: A Method for Stochastic Optimization.</em><br />
International Conference on Learning Representations (ICLR).<br />
https://arxiv.org/abs/1412.6980</p></li>
<li><p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016).<br />
<em>Deep Learning</em> (Chapter 8: Optimization for Training Deep Models).<br />
MIT Press.<br />
https://www.deeplearningbook.org</p></li>
<li><p>Loshchilov, I., &amp; Hutter, F. (2019).<br />
<em>Decoupled Weight Decay Regularization.</em><br />
International Conference on Learning Representations (ICLR).<br />
https://openreview.net/forum?id=Bkg6RiCqY7</p></li>
<li><p>Reddi, S. J., Kale, S., &amp; Kumar, S. (2018).<br />
<em>On the Convergence of Adam and Beyond.</em><br />
International Conference on Learning Representations (ICLR).<br />
https://openreview.net/forum?id=ryQu7f-RZ</p></li>
<li><p>Ruder, S. (2016).<br />
<em>An Overview of Gradient Descent Optimization Algorithms.</em><br />
arXiv:1609.04747.<br />
https://arxiv.org/abs/1609.04747</p></li>
<li><p>LeCun, Y., Cortes, C., &amp; Burges, C. J. C. (1998).<br />
<em>The MNIST database of handwritten digits.</em><br />
https://yann.lecun.com/exdb/mnist/</p></li>
<li><p>TensorFlow Keras Documentation.<br />
<em>Optimizers and training loops.</em><br />
https://www.tensorflow.org/api_docs/python/tf/keras/optimizers</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./blogpost"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../notebooks/Laboratory_6.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><strong>Laboratory Task 6 - Implementation of a CNN Architecture</strong></p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-intuition-a-tale-of-two-hikers">1. The Intuition: A Tale of Two Hikers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sgd-hiker">The SGD Hiker</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-adam-hiker">The Adam Hiker</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mathematics-under-the-hood">2. The Mathematics: Under the Hood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-moment-momentum-m-t">2.1 First Moment: Momentum <span class="math notranslate nohighlight">\(m_t\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#second-moment-variance-v-t">2.2 Second Moment: Variance <span class="math notranslate nohighlight">\(v_t\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cold-start-problem-bias-correction">2.3 The “Cold Start” Problem: Bias Correction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-update-rule">2.4 The Update Rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-adam-wins-saddle-points-sparse-gradients">3. Why Adam Wins: Saddle Points &amp; Sparse Gradients</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#escaping-saddle-points">3.1 Escaping Saddle Points</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-sparse-gradients">3.2 Handling Sparse Gradients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-experiment-adam-vs-sgd-on-mnist">4. The Experiment: Adam vs SGD on MNIST</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#experimental-setup">4.1 Experimental Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#results">4.3 Results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-observed">4.4 What We Observed</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-guide-hyperparameter-tuning-for-adam">5. Practical Guide: Hyperparameter Tuning for Adam</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-generalization-gap-warning">The <strong>“Generalization Gap”</strong> Warning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Keith Laspoña
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>