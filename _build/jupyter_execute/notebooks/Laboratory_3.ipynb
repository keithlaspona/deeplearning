{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972ba699-16a7-419a-9b7f-1fcc9ccec534",
   "metadata": {},
   "source": [
    "## **Laboratory Task 3 - Implementing Forward and Backward Propagation**\n",
    "#### **DS Elective 4 - Deep Learning**\n",
    "\n",
    "**Name:** Keith Laspo√±a <br>\n",
    "**Year & Section:** DS4A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e074262",
   "metadata": {},
   "source": [
    "**Instruction:** Perform a forward and backward propagation in python using the inputs from **Laboratory Task 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20117ccf",
   "metadata": {},
   "source": [
    "![Lab2](../notebooks/images/lab3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4095ce93",
   "metadata": {},
   "source": [
    "#### **1. Perform standard imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c584e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7824b5b",
   "metadata": {},
   "source": [
    "#### **2. Define activation functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec953df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340faa4a",
   "metadata": {},
   "source": [
    "#### **3. Initialize parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6c772dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input vector\n",
    "x = np.array([[1], [0], [1]])\n",
    "\n",
    "# Hidden unit weights\n",
    "hidden_weights = np.array([\n",
    "    [0.2, -0.3],\n",
    "    [0.4,  0.1],\n",
    "    [-0.5, 0.2]\n",
    "])\n",
    "\n",
    "# Output unit weights\n",
    "output_weights = np.array([\n",
    "    [-0.3],\n",
    "    [-0.2]\n",
    "])\n",
    "\n",
    "# Bias\n",
    "hidden_bias = np.array([[-0.4], [0.2]])\n",
    "output_bias = np.array([[0.1]])\n",
    "\n",
    "# True target value\n",
    "target = np.array([[1]])\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ebdcd9",
   "metadata": {},
   "source": [
    "#### **4. Forward Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc3bfb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prediction: 0.0800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate hidden layer weighted sum\n",
    "hidden_sum = hidden_weights.T @ x + hidden_bias\n",
    "# Activate hidden layer neurons\n",
    "hidden_output = relu(hidden_sum)\n",
    "\n",
    "# Calculate output layer weighted sum\n",
    "output_sum = output_weights.T @ hidden_output + output_bias\n",
    "# Get final prediction\n",
    "prediction = relu(output_sum)\n",
    "\n",
    "print(f\"Final Prediction: {prediction.flatten()[0]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d5fbb",
   "metadata": {},
   "source": [
    "#### **5. Backward Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc74b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta for output layer (delta_output):\n",
      "[[-0.92]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate error derivative\n",
    "error_derivative = -(target - prediction)\n",
    "\n",
    "# Backpropagate to output layer\n",
    "delta_output = error_derivative * relu_derivative(output_sum)\n",
    "print(f\"Delta for output layer (delta_output):\\n{delta_output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf2a988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of error wrt output weights (d_output_weights):\n",
      "[[ 0.    -0.092]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate derivative of error wrt output weights\n",
    "d_output_weights = delta_output @ hidden_output.T\n",
    "print(f\"Derivative of error wrt output weights (d_output_weights):\\n{d_output_weights}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e85c80f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta for hidden layer (delta_hidden):\n",
      "[[0.   ]\n",
      " [0.184]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Backpropagate to hidden layer\n",
    "delta_hidden = (output_weights @ delta_output) * relu_derivative(hidden_sum)\n",
    "print(f\"Delta for hidden layer (delta_hidden):\\n{delta_hidden}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18186a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of error wrt hidden weights (d_hidden_weights):\n",
      "[[0.    0.    0.   ]\n",
      " [0.184 0.    0.184]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate derivative of error wrt hidden weights\n",
    "d_hidden_weights = delta_hidden @ x.T\n",
    "print(f\"Derivative of error wrt hidden weights (d_hidden_weights):\\n{d_hidden_weights}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99206e11",
   "metadata": {},
   "source": [
    "#### **6. Update Weights and Biases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e0f1fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Output Weights (output_weights):\n",
      "[[-0.3     ]\n",
      " [-0.199908]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update output layer weights and bias\n",
    "output_weights -= lr * d_output_weights.T\n",
    "output_bias -= lr * delta_output\n",
    "\n",
    "print(f\"Updated Output Weights (output_weights):\\n{output_weights}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7517ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Hidden Weights (hidden_weights):\n",
      "[[ 0.2      -0.300184]\n",
      " [ 0.4       0.1     ]\n",
      " [-0.5       0.199816]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update hidden layer weights and bias\n",
    "hidden_weights -= lr * d_hidden_weights.T\n",
    "hidden_bias -= lr * delta_hidden\n",
    "\n",
    "print(f\"Updated Hidden Weights (hidden_weights):\\n{hidden_weights}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414c6bd",
   "metadata": {},
   "source": [
    "#### **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bcda39",
   "metadata": {},
   "source": [
    "In this laboratory, we successfully implemented a complete training cycle for a simple neural network. The process commenced with **forward propagation**, where the initial inputs were passed through the hidden and output layers to generate a final prediction of **0.0800**.\n",
    "\n",
    "Subsequently, we initiated **backward propagation** by first calculating the error between this prediction and the true target value. This error signal was then propagated backward through the network, allowing us to compute the gradients or the contribution of each weight and bias to the total error using the chain rule. These gradients indicated the direction and magnitude of change required for each parameter to improve the model's performance. \n",
    "\n",
    "\n",
    "The final step was the **parameter update**, where we adjusted the network's weights and biases in the opposite direction of their respective gradients, scaled by a small learning rate. This single, complete iteration demonstrates the fundamental mechanism by which a neural network learns, incrementally refining its parameters to minimize prediction error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}